{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea9eda70-6bb1-47bb-bbea-30ff656b5906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Clean up tables, schemas, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf37cfe-7735-4a08-b649-11a44bb604ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üßπ CLEANUP CELL ‚Äî Drop tables, schema, and/or physical paths\n",
    "# ================================================================\n",
    "\n",
    "# ==== CONFIGURE THESE ==== \n",
    "drop_tables = \"yes\"     # \"yes\" / \"no\" ‚Äî Drop tables\n",
    "drop_schema = \"yes\"      # \"yes\" / \"no\" ‚Äî Drop schema\n",
    "drop_paths  = \"yes\"      # \"yes\" / \"no\" ‚Äî Delete all data folders (irreversible!)\n",
    "\n",
    "catalog = \"main\"        # or \"hive_metastore\" if not using Unity Catalog\n",
    "schema  = \"demo_schema_migration\"\n",
    "table   = \"bronze_customers_demo\"\n",
    "\n",
    "full_table = f\"{catalog}.{schema}.{table}\"\n",
    "\n",
    "# ==== 1Ô∏è‚É£ DROP TABLE(S) =======================================================\n",
    "if drop_tables.lower() == \"yes\":\n",
    "    print(f\"üóÇ Dropping table if exists: {full_table}\")\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {full_table}\")\n",
    "        print(\"   ‚úÖ Table dropped (or didn‚Äôt exist)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è DROP TABLE failed: {e}\")\n",
    "else:\n",
    "    print(\"üö´ Skipping table drop (drop_tables != 'yes')\")\n",
    "\n",
    "# ==== 2Ô∏è‚É£ DROP SCHEMA =========================================================\n",
    "if drop_schema.lower() == \"yes\":\n",
    "    print(f\"üè∑  Dropping schema if exists: {catalog}.{schema}\")\n",
    "    try:\n",
    "        spark.sql(f\"USE CATALOG {catalog}\")\n",
    "        # UC: DROP SCHEMA, HMS: DROP DATABASE ‚Äî both accept CASCADE in Databricks\n",
    "        spark.sql(f\"DROP SCHEMA IF EXISTS {schema} CASCADE\")\n",
    "        print(\"   ‚úÖ Dropped schema (and all contained objects)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è DROP SCHEMA failed: {e}\")\n",
    "else:\n",
    "    print(\"üö´ Skipping schema drop (drop_schema != 'yes')\")\n",
    "\n",
    "# ==== 3Ô∏è‚É£ DELETE PHYSICAL PATHS ==============================================\n",
    "if drop_paths.lower() == \"yes\":\n",
    "    print(\"üóë Deleting data and auxiliary folders...\")\n",
    "    paths_to_delete = [raw_path, schema_loc_path, delta_path, chk1, chk2, base_path]\n",
    "    for p in paths_to_delete:\n",
    "        try:\n",
    "            print(f\"   ‚ûú Deleting: {p}\")\n",
    "            dbutils.fs.rm(p, recurse=True)\n",
    "            print(\"      ‚úÖ Deleted successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è Could not delete {p}: {e}\")\n",
    "else:\n",
    "    print(\"üö´ Skipping path deletion (drop_paths != 'yes')\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup completed (based on selected parameters).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4aa9fb-46c9-4a8f-9163-203ae7bf631d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1Ô∏è‚É£ Environment Setup & SAS Authentication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7931b256-0c4a-4974-9570-db925e806b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==== EDIT THESE ==== # <-- Section you customize per environment/run\n",
    "\n",
    "storage_account = \"peterunitystorage\"      \n",
    "# Azure Storage Account name (ADLS Gen2). Needed to build abfss:// URLs.\n",
    "container       = \"peterunitycontainer\"\n",
    "# ADLS Gen2 container (filesystem) that holds your data & table files.\n",
    "sas_token       = \"?sp=...&se=...&sig=...\"\n",
    "# SAS token granting time-boxed access. Lets Spark read/write without AAD creds.\n",
    "\n",
    "# Hive DB name (non-UC). If UC, just USE main.<schema>\n",
    "schema_name     = \"demo_schema_migration\"\n",
    "# Target database/schema for CREATE TABLE (legacy Hive metastore).\n",
    "table_name      = \"bronze_customers_demo\"    \n",
    "# Logical table name; also used to derive storage folder paths below.\n",
    "\n",
    "# Spark auth for SAS (Azure ADLS Gen2)\n",
    "# Tell Hadoop ABFS driver to authenticate to this storage account using SAS.\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\"\n",
    ")\n",
    "\n",
    "# Use a fixed (pre-generated) SAS token provider at runtime.\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "  \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n",
    ")\n",
    "\n",
    "# The actual SAS token value used by the provider above.\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)\n",
    "\n",
    "# Paths\n",
    "# Root folder for this dataset/table in ADLS Gen2 using the ABFS(S) scheme.\n",
    "base_path       = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{table_name}\"\n",
    "\n",
    "# Landing zone for raw source files (e.g., JSON/CSV) per table.\n",
    "raw_path        = f\"{base_path}/raw\"\n",
    "\n",
    "# Auto Loader schema evolution store (_schemas). Tracks inferred/updated schema.\n",
    "schema_loc_path = f\"{base_path}/_schemas\"\n",
    "\n",
    "# Where the Delta Lake table files (parquet + _delta_log) are written.\n",
    "delta_path      = f\"{base_path}/delta\"       \n",
    "\n",
    "# Streaming checkpoint dir for pipeline version 1 (offsets, progress, state).\n",
    "chk1            = f\"{base_path}/_chk_v1\"\n",
    "# A separate checkpoint for an alternate pipeline/version to avoid conflicts.\n",
    "chk2            = f\"{base_path}/_chk_v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "988f335e-72a0-47d0-8ce3-9cd38de586ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1Ô∏è‚É£/A Inspect All Paths and Show Their Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2fffab-1ca1-4e8a-9c34-32a2cad70139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# INSPECT STORAGE STRUCTURE FOR CURRENT TABLE PIPELINE\n",
    "# ================================================================\n",
    "\n",
    "# Show the paths used in this environment\n",
    "print(\"=== PATHS CONFIGURED ===\")\n",
    "print(f\"Base Path        : {base_path}\")\n",
    "print(f\"Raw Path         : {raw_path}\")\n",
    "print(f\"Schema Log Path  : {schema_loc_path}\")\n",
    "print(f\"Delta Table Path : {delta_path}\")\n",
    "print(f\"Checkpoint v1    : {chk1}\")\n",
    "print(f\"Checkpoint v2    : {chk2}\")\n",
    "print(\"=========================================\\n\")\n",
    "\n",
    "# Helper function to list a folder's contents safely\n",
    "def show_folder(path: str):\n",
    "    print(f\"\\nüìÅ {path}\")\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        if len(files) == 0:\n",
    "            print(\"   (empty)\")\n",
    "        else:\n",
    "            for f in files:\n",
    "                print(f\"   {f.name:<40}  {f.size/1024:.1f} KB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Could not list: {e}\")\n",
    "\n",
    "# List each important folder\n",
    "for p in [base_path, raw_path, schema_loc_path, delta_path, chk1, chk2]:\n",
    "    show_folder(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97c99c99-20b0-4529-8715-94df3fe097b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop the table(s) first (idempotent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d7fb26b-f6c9-4055-b2e3-1dfac95d58be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adjust if your table/schema names differ\n",
    "catalog = \"main\"  # or \"hive_metastore\" if you used legacy HMS\n",
    "schema  = \"demo_schema_migration\"\n",
    "table   = \"bronze_customers_demo\"\n",
    "\n",
    "full_table = f\"{catalog}.{schema}.{table}\"\n",
    "\n",
    "print(f\"üóÇ Dropping table if exists: {full_table}\")\n",
    "try:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {full_table}\")\n",
    "    print(\"   ‚úÖ Dropped (or didn‚Äôt exist)\")\n",
    "except Exception as e:\n",
    "    print(\"   ‚ö†Ô∏è DROP TABLE failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f3ebdff-d146-4fbe-a85f-91439536ca9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop the schema (catalog) entry (idempotent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93822d55-775c-4f92-ae3f-80f203522cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"main\"  # or \"hive_metastore\"\n",
    "schema  = \"demo_schema_migration\"\n",
    "\n",
    "print(f\"üè∑  Dropping schema if exists: {catalog}.{schema}\")\n",
    "try:\n",
    "    spark.sql(f\"USE CATALOG {catalog}\")\n",
    "    # UC uses DROP SCHEMA; HMS uses DROP DATABASE ‚Äî both accept CASCADE on Databricks\n",
    "    spark.sql(f\"DROP SCHEMA IF EXISTS {schema} CASCADE\")\n",
    "    print(\"   ‚úÖ Dropped schema (and all catalog objects within)\")\n",
    "except Exception as e:\n",
    "    print(\"   ‚ö†Ô∏è DROP SCHEMA failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f86a920-22f2-40c8-9295-abe842766736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "702a06d1-db0d-4800-9eee-ea1169820c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "paths_to_delete = [raw_path, schema_loc_path, delta_path, chk1, chk2, base_path]\n",
    "\n",
    "for p in paths_to_delete:\n",
    "    try:\n",
    "        print(f\"üóë Deleting: {p}\")\n",
    "        dbutils.fs.rm(p, recurse=True)\n",
    "        print(\"   ‚úÖ Deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not delete {p}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ecc6e8-8365-4ae1-b226-547675f1f9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2Ô∏è‚É£ Prepare Sample Input Files (v1 & v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6e87fc-528d-4bc1-86f1-e7273311aabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: create small JSON datasets\n",
    "# v1 (base schema)\n",
    "# v2 (adds new column, bad type, unexpected column).\n",
    "\n",
    "# v1\n",
    "dbutils.fs.mkdirs(f\"{raw_path}/v1\")\n",
    "dbutils.fs.put(f\"{raw_path}/v1/customers_1.json\", \"\"\"\n",
    "{\"id\":1,\"name\":\"Alice\",\"age\":31}\n",
    "{\"id\":2,\"name\":\"Bob\",\"age\":28}\n",
    "\"\"\", True)\n",
    "\n",
    "# v2 (new column + type mismatch + extra unexpected column)\n",
    "dbutils.fs.mkdirs(f\"{raw_path}/v2\")\n",
    "dbutils.fs.put(f\"{raw_path}/v2/customers_2.json\", \"\"\"\n",
    "{\"id\":3,\"name\":\"Chao\",\"age\":35,\"email\":\"chao@example.com\"}\n",
    "{\"id\":4,\"name\":\"Dana\",\"age\":\"unknown\",\"email\":\"dana@example.com\",\"country\":\"PL\"}\n",
    "\"\"\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b343217c-fe64-4dbd-81ff-a48a45ba7b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3Ô∏è‚É£ Auto Loader ‚Äì Initial Ingest (v1, v2 ‚Üí Delta Bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682cbedc-785e-4334-a34d-f3f2b965f3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, lit\n",
    "\n",
    "# --- Config you already have ---\n",
    "# raw_path, delta_path, table_name, schema_loc_path, chk1, chk2 must exist\n",
    "\n",
    "# Optional: register the Delta table (idempotent)\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{delta_path}'\")\n",
    "\n",
    "# Describe the two independent ingests\n",
    "ingests = [\n",
    "    {\n",
    "        \"version\": \"v1\",\n",
    "        \"source_path\": f\"{raw_path}/v1\",\n",
    "        \"checkpoint\": chk1,\n",
    "        \"add_source_file\": True,\n",
    "    },\n",
    "    {\n",
    "        \"version\": \"v2\",\n",
    "        \"source_path\": f\"{raw_path}/v2\",\n",
    "        \"checkpoint\": chk2,\n",
    "        \"add_source_file\": False,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run each ingest sequentially with trigger(once)\n",
    "for cfg in ingests:\n",
    "    stream_df = (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.schemaLocation\", schema_loc_path)      \n",
    "            # shared schema log (fine)\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "            .option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "            .load(cfg[\"source_path\"])\n",
    "            .withColumn(\"_ingest_version\", lit(cfg[\"version\"]))\n",
    "    )\n",
    "    if cfg[\"add_source_file\"]:\n",
    "        stream_df = stream_df.withColumn(\"_source_file\", input_file_name())\n",
    "\n",
    "    q = (\n",
    "        stream_df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", cfg[\"checkpoint\"])\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(once=True)\n",
    "            .start(delta_path)\n",
    "    )\n",
    "    q.awaitTermination()  # ensures v1 finishes before v2 starts (or vice versa)\n",
    "\n",
    "# Show results\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd49d87-6b66-4725-9167-e6c8c17942b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4Ô∏è‚É£ Auto Loader ‚Äì Schema Evolution & Rescue Mode (v2 ‚Üí Delta Bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7482036c-23d9-4dff-8817-93373ec3a762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5Ô∏è‚É£ Schema Enforcement ‚Äì Delta Rejects Incompatible Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef069aa-72b2-4fc2-92ed-1a83c67f5dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df_newcol = spark.createDataFrame([Row(id=5, name=\"Eve\", age=29, vip=True)])\n",
    "\n",
    "try:\n",
    "    (df_newcol.write.format(\"delta\").mode(\"append\").save(delta_path))  # no mergeSchema\n",
    "except Exception as e:\n",
    "    print(\"Expected failure (schema enforcement):\\n\", str(e)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aac1b80-e598-4f78-abd9-e777e066c98b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6Ô∏è‚É£ Schema Evolution ‚Äì Add New Column with mergeSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6894e40-74ce-459a-8495-eeba0f71203e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df_newcol.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"append\")\n",
    "   .option(\"mergeSchema\",\"true\")\n",
    "   .save(delta_path))\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f97b361-6082-4361-987d-5533d24eb73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7Ô∏è‚É£ Schema Merging ‚Äì Union Two Sources with Different Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6593b113-469f-47a4-bbc4-165084403d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Build two different DataFrames with non-matching schemas\n",
    "\n",
    "df_a = spark.createDataFrame([Row(id=6, name=\"Fran\", age=40)])\n",
    "df_b = spark.createDataFrame([Row(id=7, name=\"G√∂ran\", email=\"g@example.com\")])\n",
    "\n",
    "# Union-merge these schemas >\n",
    "# allowMissingColumns=True tells to allow Missing columns and Spark fills them Null\n",
    "# For columns that exist in both inputs >> their data types must be compatible \n",
    "# (Spark may upcast, but it won‚Äôt merge fundamentally incompatible types)\n",
    "\n",
    "union_merged = df_a.unionByName(df_b, allowMissingColumns=True)\n",
    "# Spark has two main kinds of ‚Äúunion‚Äù operations:\n",
    "# DataFrame.union() >> Stacks two DataFrames row-wise. Columns are matched by position (order must match). Same number of columns and compatible data types required.\n",
    "# DataFrame.unionByName() >> Stacks two DataFrames row-wise, but aligns columns by name instead of position. Can optionally allow missing columns (allowMissingColumns=True) to fill nulls automatically.\n",
    "\n",
    "# Result merged schema: id, name, age, email.\n",
    "# where Row from df_a ‚Üí email = null\n",
    "# where Row from df_b ‚Üí age = null\n",
    "\n",
    "# Append union-merged dataframe to an existing Delta table at delta_path.\n",
    "(union_merged.write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"append\")\n",
    "  .option(\"mergeSchema\",\"true\") \n",
    "  .save(delta_path))\n",
    "\n",
    "# .option(\"mergeSchema\",\"true\") >> lets Delta evolve the table schema to include new columns present in the DataFrame (here: email), instead of throwing a schema mismatch error.\n",
    "# Delta will preserve existing columns‚Äô order and append new columns at the end; the new column(s) will be nullable.\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ca1831-c34b-408a-9ead-2168d3e1732f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8Ô∏è‚É£ Normalize Schema Before Union (Handling Type Mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3135c271-559c-4d27-865f-067edb96a34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, lit\n",
    "\n",
    "# Read raw JSON\n",
    "df_a = spark.read.json(f\"{raw_path}/v1\")  # has: id, name, age (numeric)\n",
    "df_b = spark.read.json(f\"{raw_path}/v2\")  # has: id, name, age (sometimes \"unknown\"), email\n",
    "\n",
    "# 1) Normalize schemas BEFORE union:\n",
    "# - df_a: add missing 'email'\n",
    "# - both: cast age safely (string->long), non-numeric => NULL\n",
    "df_a_norm = (\n",
    "    df_a\n",
    "    .withColumn(\"email\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"age\", expr(\"try_cast(age as long)\"))\n",
    ")\n",
    "\n",
    "df_b_norm = (\n",
    "    df_b\n",
    "    .withColumn(\"age\", expr(\"try_cast(age as long)\"))\n",
    "    # ensure email exists & is string (already is, but explicit for clarity)\n",
    "    .withColumn(\"email\", expr(\"cast(email as string)\"))\n",
    ")\n",
    "\n",
    "# 2) Union by name allowing missing columns (now types align)\n",
    "merged = df_a_norm.unionByName(df_b_norm, allowMissingColumns=True)\n",
    "\n",
    "# 3) Write back (age stays BIGINT; 'unknown' becomes NULL)\n",
    "(merged.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")            # overwrite for a clean combined result\n",
    "   .option(\"mergeSchema\",\"true\") # allow adding 'email' if not present\n",
    "   .save(delta_path))\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Schema enforcement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
