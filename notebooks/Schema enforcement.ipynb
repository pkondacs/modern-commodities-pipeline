{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4aa9fb-46c9-4a8f-9163-203ae7bf631d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## NEW VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7931b256-0c4a-4974-9570-db925e806b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==== EDIT THESE ====\n",
    "storage_account = \"peterunitystorage\"\n",
    "container       = \"peterunitycontainer\"\n",
    "sas_token       = \"?sp=racwdlmeop&st=2025-10-13T14:01:09Z&se=2025-10-29T23:16:09Z&spr=https&sv=2024-11-04&sr=c&sig=XuhEQOnoFpIQj47x26OSPKo87F4ZGfOL%2BYyJ69ebga8%3D\"\n",
    "# Hive DB name (non-UC). If UC, just USE main.<schema>\n",
    "schema_name     = \"demo_schema_migration\" \n",
    "table_name      = \"bronze_customers_demo\"\n",
    "\n",
    "# Spark auth for SAS (Azure ADLS Gen2)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)\n",
    "\n",
    "# Paths\n",
    "base_path       = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{table_name}\"\n",
    "raw_path        = f\"{base_path}/raw\"\n",
    "schema_loc_path = f\"{base_path}/_schemas\"\n",
    "delta_path      = f\"{base_path}/delta\"\n",
    "chk1            = f\"{base_path}/_chk_v1\"\n",
    "chk2            = f\"{base_path}/_chk_v2\"\n",
    "\n",
    "# DB (Hive metastore for non-UC)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name}\")\n",
    "spark.sql(f\"USE {schema_name}\")\n",
    "\n",
    "# Clean up prior runs\n",
    "for p in [f\"{base_path}/\",]:\n",
    "    try: dbutils.fs.rm(p, True)\n",
    "    except: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a51bf35-fa78-4dc5-95f4-40608ad4e57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List out the folder contents\n",
    "display(dbutils.fs.ls(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6e87fc-528d-4bc1-86f1-e7273311aabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# v1\n",
    "dbutils.fs.mkdirs(f\"{raw_path}/v1\")\n",
    "dbutils.fs.put(f\"{raw_path}/v1/customers_1.json\", \"\"\"\n",
    "{\"id\":1,\"name\":\"Alice\",\"age\":31}\n",
    "{\"id\":2,\"name\":\"Bob\",\"age\":28}\n",
    "\"\"\", True)\n",
    "\n",
    "# v2 (new column + type mismatch + extra unexpected column)\n",
    "dbutils.fs.mkdirs(f\"{raw_path}/v2\")\n",
    "dbutils.fs.put(f\"{raw_path}/v2/customers_2.json\", \"\"\"\n",
    "{\"id\":3,\"name\":\"Chao\",\"age\":35,\"email\":\"chao@example.com\"}\n",
    "{\"id\":4,\"name\":\"Dana\",\"age\":\"unknown\",\"email\":\"dana@example.com\",\"country\":\"PL\"}\n",
    "\"\"\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa7ce5b9-9c1d-41f1-a777-1d02b0c6db1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "stream_df_v1 = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\",\"json\")\n",
    "  .option(\"cloudFiles.schemaLocation\", schema_loc_path)\n",
    "  .option(\"cloudFiles.inferColumnTypes\",\"true\")\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\",\"rescue\")  # capture surprises\n",
    "  .option(\"rescuedDataColumn\",\"_rescued_data\")\n",
    "  .load(f\"{raw_path}/v1\")\n",
    "  .withColumn(\"_source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "q1 = (stream_df_v1.writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", chk1)\n",
    "  .option(\"mergeSchema\",\"true\")\n",
    "  .outputMode(\"append\")\n",
    "  .trigger(once=True)\n",
    "  .start(delta_path))\n",
    "q1.awaitTermination()\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{delta_path}'\")\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c72ea58-1292-4e78-88fb-243e3e8df4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df_v2 = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\",\"json\")\n",
    "  .option(\"cloudFiles.schemaLocation\", schema_loc_path)\n",
    "  .option(\"cloudFiles.inferColumnTypes\",\"true\")\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\",\"rescue\")\n",
    "  .option(\"rescuedDataColumn\",\"_rescued_data\")\n",
    "  .load(f\"{raw_path}/v2\"))\n",
    "\n",
    "q2 = (stream_df_v2.writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", chk2)\n",
    "  .option(\"mergeSchema\",\"true\")\n",
    "  .outputMode(\"append\")\n",
    "  .trigger(once=True)\n",
    "  .start(delta_path))\n",
    "q2.awaitTermination()\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef069aa-72b2-4fc2-92ed-1a83c67f5dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df_newcol = spark.createDataFrame([Row(id=5, name=\"Eve\", age=29, vip=True)])\n",
    "\n",
    "try:\n",
    "    (df_newcol.write.format(\"delta\").mode(\"append\").save(delta_path))  # no mergeSchema\n",
    "except Exception as e:\n",
    "    print(\"Expected failure (schema enforcement):\\n\", str(e)[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6894e40-74ce-459a-8495-eeba0f71203e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df_newcol.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"append\")\n",
    "   .option(\"mergeSchema\",\"true\")\n",
    "   .save(delta_path))\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6593b113-469f-47a4-bbc4-165084403d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df_a = spark.createDataFrame([Row(id=6, name=\"Fran\", age=40)])\n",
    "df_b = spark.createDataFrame([Row(id=7, name=\"GÃ¶ran\", email=\"g@example.com\")])\n",
    "\n",
    "merged = df_a.unionByName(df_b, allowMissingColumns=True)\n",
    "\n",
    "(merged.write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"append\")\n",
    "  .option(\"mergeSchema\",\"true\")\n",
    "  .save(delta_path))\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47ca1831-c34b-408a-9ead-2168d3e1732f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Normalize schema before union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3135c271-559c-4d27-865f-067edb96a34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, lit\n",
    "\n",
    "# Read raw JSON\n",
    "df_a = spark.read.json(f\"{raw_path}/v1\")  # has: id, name, age (numeric)\n",
    "df_b = spark.read.json(f\"{raw_path}/v2\")  # has: id, name, age (sometimes \"unknown\"), email\n",
    "\n",
    "# 1) Normalize schemas BEFORE union:\n",
    "# - df_a: add missing 'email'\n",
    "# - both: cast age safely (string->long), non-numeric => NULL\n",
    "df_a_norm = (\n",
    "    df_a\n",
    "    .withColumn(\"email\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"age\", expr(\"try_cast(age as long)\"))\n",
    ")\n",
    "\n",
    "df_b_norm = (\n",
    "    df_b\n",
    "    .withColumn(\"age\", expr(\"try_cast(age as long)\"))\n",
    "    # ensure email exists & is string (already is, but explicit for clarity)\n",
    "    .withColumn(\"email\", expr(\"cast(email as string)\"))\n",
    ")\n",
    "\n",
    "# 2) Union by name allowing missing columns (now types align)\n",
    "merged = df_a_norm.unionByName(df_b_norm, allowMissingColumns=True)\n",
    "\n",
    "# 3) Write back (age stays BIGINT; 'unknown' becomes NULL)\n",
    "(merged.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")            # overwrite for a clean combined result\n",
    "   .option(\"mergeSchema\",\"true\") # allow adding 'email' if not present\n",
    "   .save(delta_path))\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Schema enforcement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
