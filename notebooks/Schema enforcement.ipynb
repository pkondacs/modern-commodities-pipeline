{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4aa9fb-46c9-4a8f-9163-203ae7bf631d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1️⃣ Environment Setup & SAS Authentication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7931b256-0c4a-4974-9570-db925e806b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Configure Azure ADLS Gen2 access using SAS token, define paths, database, and clean up any previous run.\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "storage_account = \"peterunitystorage\"\n",
    "container       = \"peterunitycontainer\"\n",
    "sas_token       = \"?sp=racwdlmeop&st=2025-10-13T14:01:09Z&se=2025-10-29T23:16:09Z&spr=https&sv=2024-11-04&sr=c&sig=XuhEQOnoFpIQj47x26OSPKo87F4ZGfOL%2BYyJ69ebga8%3D\"\n",
    "# Hive DB name (non-UC). If UC, just USE main.<schema>\n",
    "schema_name     = \"demo_schema_migration\" \n",
    "table_name      = \"bronze_customers_demo\"\n",
    "\n",
    "# Spark auth for SAS (Azure ADLS Gen2)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)\n",
    "\n",
    "# Paths\n",
    "base_path       = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{table_name}\"\n",
    "raw_path        = f\"{base_path}/raw\"\n",
    "schema_loc_path = f\"{base_path}/_schemas\"\n",
    "delta_path      = f\"{base_path}/delta\"\n",
    "chk1            = f\"{base_path}/_chk_v1\"\n",
    "chk2            = f\"{base_path}/_chk_v2\"\n",
    "\n",
    "# DB (Hive metastore for non-UC)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name}\")\n",
    "spark.sql(f\"USE {schema_name}\")\n",
    "\n",
    "# Clean up prior runs\n",
    "for p in [f\"{base_path}/\",]:\n",
    "    try: dbutils.fs.rm(p, True)\n",
    "    except: pass\n",
    "\n",
    "# List out the folder contents\n",
    "display(dbutils.fs.ls(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ecc6e8-8365-4ae1-b226-547675f1f9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2️⃣ Prepare Sample Input Files (v1 & v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6e87fc-528d-4bc1-86f1-e7273311aabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: create small JSON datasets\n",
    "# v1 (base schema)\n",
    "# v2 (adds new column, bad type, unexpected column).\n",
    "\n",
    "# v1\n",
    "dbutils.fs.mkdirs(f\"{raw_path}/v1\")\n",
    "dbutils.fs.put(f\"{raw_path}/v1/customers_1.json\", \"\"\"\n",
    "{\"id\":1,\"name\":\"Alice\",\"age\":31}\n",
    "{\"id\":2,\"name\":\"Bob\",\"age\":28}\n",
    "\"\"\", True)\n",
    "\n",
    "# v2 (new column + type mismatch + extra unexpected column)\n",
    "dbutils.fs.mkdirs(f\"{raw_path}/v2\")\n",
    "dbutils.fs.put(f\"{raw_path}/v2/customers_2.json\", \"\"\"\n",
    "{\"id\":3,\"name\":\"Chao\",\"age\":35,\"email\":\"chao@example.com\"}\n",
    "{\"id\":4,\"name\":\"Dana\",\"age\":\"unknown\",\"email\":\"dana@example.com\",\"country\":\"PL\"}\n",
    "\"\"\", True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b343217c-fe64-4dbd-81ff-a48a45ba7b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3️⃣ Auto Loader – Initial Ingest (v1, v2 → Delta Bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682cbedc-785e-4334-a34d-f3f2b965f3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, lit\n",
    "\n",
    "# --- Config you already have ---\n",
    "# raw_path, delta_path, table_name, schema_loc_path, chk1, chk2 must exist\n",
    "\n",
    "# Optional: register the Delta table (idempotent)\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{delta_path}'\")\n",
    "\n",
    "# Describe the two independent ingests\n",
    "ingests = [\n",
    "    {\n",
    "        \"version\": \"v1\",\n",
    "        \"source_path\": f\"{raw_path}/v1\",\n",
    "        \"checkpoint\": chk1,\n",
    "        \"add_source_file\": True,\n",
    "    },\n",
    "    {\n",
    "        \"version\": \"v2\",\n",
    "        \"source_path\": f\"{raw_path}/v2\",\n",
    "        \"checkpoint\": chk2,\n",
    "        \"add_source_file\": False,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run each ingest sequentially with trigger(once)\n",
    "for cfg in ingests:\n",
    "    stream_df = (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.schemaLocation\", schema_loc_path)      # shared schema log (fine)\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "            .option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "            .load(cfg[\"source_path\"])\n",
    "            .withColumn(\"_ingest_version\", lit(cfg[\"version\"]))\n",
    "    )\n",
    "    if cfg[\"add_source_file\"]:\n",
    "        stream_df = stream_df.withColumn(\"_source_file\", input_file_name())\n",
    "\n",
    "    q = (\n",
    "        stream_df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", cfg[\"checkpoint\"])\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(once=True)\n",
    "            .start(delta_path)\n",
    "    )\n",
    "    q.awaitTermination()  # ensures v1 finishes before v2 starts (or vice versa)\n",
    "\n",
    "# Show results\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd49d87-6b66-4725-9167-e6c8c17942b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4️⃣ Auto Loader – Schema Evolution & Rescue Mode (v2 → Delta Bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7482036c-23d9-4dff-8817-93373ec3a762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5️⃣ Schema Enforcement – Delta Rejects Incompatible Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef069aa-72b2-4fc2-92ed-1a83c67f5dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df_newcol = spark.createDataFrame([Row(id=5, name=\"Eve\", age=29, vip=True)])\n",
    "\n",
    "try:\n",
    "    (df_newcol.write.format(\"delta\").mode(\"append\").save(delta_path))  # no mergeSchema\n",
    "except Exception as e:\n",
    "    print(\"Expected failure (schema enforcement):\\n\", str(e)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aac1b80-e598-4f78-abd9-e777e066c98b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6️⃣ Schema Evolution – Add New Column with mergeSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6894e40-74ce-459a-8495-eeba0f71203e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df_newcol.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"append\")\n",
    "   .option(\"mergeSchema\",\"true\")\n",
    "   .save(delta_path))\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f97b361-6082-4361-987d-5533d24eb73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7️⃣ Schema Merging – Union Two Sources with Different Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6593b113-469f-47a4-bbc4-165084403d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Build two different DataFrames with non-matching schemas\n",
    "\n",
    "df_a = spark.createDataFrame([Row(id=6, name=\"Fran\", age=40)])\n",
    "df_b = spark.createDataFrame([Row(id=7, name=\"Göran\", email=\"g@example.com\")])\n",
    "\n",
    "# Merge these schemas >\n",
    "# allowMissingColumns=True tells to allow Missing columns and Spark fills them Null\n",
    "# For columns that exist in both inputs >> their data types must be compatible (Spark may upcast, but it won’t merge fundamentally incompatible types)\n",
    "\n",
    "merged = df_a.unionByName(df_b, allowMissingColumns=True)\n",
    "\n",
    "# Result merged schema: id, name, age, email.\n",
    "# where Row from df_a → email = null\n",
    "# where Row from df_b → age = null\n",
    "\n",
    "# Append merged dataframe to an existing Delta table at delta_path.\n",
    "(merged.write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"append\")\n",
    "  .option(\"mergeSchema\",\"true\") \n",
    "  .save(delta_path))\n",
    "\n",
    "# .option(\"mergeSchema\",\"true\") >> lets Delta evolve the table schema to include new columns present in the DataFrame (here: email), instead of throwing a schema mismatch error.\n",
    "# Delta will preserve existing columns’ order and append new columns at the end; the new column(s) will be nullable.\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ca1831-c34b-408a-9ead-2168d3e1732f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8️⃣ Normalize Schema Before Union (Handling Type Mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3135c271-559c-4d27-865f-067edb96a34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, lit\n",
    "\n",
    "# Read raw JSON\n",
    "df_a = spark.read.json(f\"{raw_path}/v1\")  # has: id, name, age (numeric)\n",
    "df_b = spark.read.json(f\"{raw_path}/v2\")  # has: id, name, age (sometimes \"unknown\"), email\n",
    "\n",
    "# 1) Normalize schemas BEFORE union:\n",
    "# - df_a: add missing 'email'\n",
    "# - both: cast age safely (string->long), non-numeric => NULL\n",
    "df_a_norm = (\n",
    "    df_a\n",
    "    .withColumn(\"email\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"age\", expr(\"try_cast(age as long)\"))\n",
    ")\n",
    "\n",
    "df_b_norm = (\n",
    "    df_b\n",
    "    .withColumn(\"age\", expr(\"try_cast(age as long)\"))\n",
    "    # ensure email exists & is string (already is, but explicit for clarity)\n",
    "    .withColumn(\"email\", expr(\"cast(email as string)\"))\n",
    ")\n",
    "\n",
    "# 2) Union by name allowing missing columns (now types align)\n",
    "merged = df_a_norm.unionByName(df_b_norm, allowMissingColumns=True)\n",
    "\n",
    "# 3) Write back (age stays BIGINT; 'unknown' becomes NULL)\n",
    "(merged.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")            # overwrite for a clean combined result\n",
    "   .option(\"mergeSchema\",\"true\") # allow adding 'email' if not present\n",
    "   .save(delta_path))\n",
    "\n",
    "display(spark.read.table(table_name).orderBy(\"id\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Schema enforcement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
